RocksDB - Useful () JVM - Off heap memory
1. Rocksdb runs in the same process as app code 
2. Each app will have its own rocks db
3. Isolation - race conditions will not happen
4. No coordination (building systems that avoid )
5. Local access is very fast without n/w call (Local disk)
6. Total control over the system. 

Disadvantages

1. Monitoring & tuning is our responsibility
2. No persistant storage
3. No failover or no availability & no distributed
4. More of a system building block


Inmemory cache or sqllite3

But scales beyond instance memory

Application patterns-

1. Sharded kafka/stream processor - Blackboard

App storage - 40 gb max, save checkpoints to TOSS

2. Resilient READ-ONLY apis
 
Alternative to autobahn, expands beyond memory.

3. Disk-Backed OFF Heap Cache


Anti - Application patterns-

1. Not a source of truth (Postgres/Cassandra/Mongo)

API's

1. No types and schemas. 
2. Keys and Values are byte arrays
3. Get and PUT
4. Batch
5. Iterator - Key sort order is important to understand when iterating
6. snapshots - zipped or tar'ed to TOSS
7. ColumnFamiles - similar to table that can be tuned individually
8. Transactions - Persist all keys/values or none
9. ttl - Not that great

Architectural 

BTREE - Optimized for reading but not writing

LSM Tree - Log structured merge tree - nosql 

WAL - 
Mem Tables
SST files 

SST files imp - 
 1. LMS Building block. Stored string table
 2.  Sorted immutable maps of keys and values
 3. keys and values are arbitary byte arrays  Compaction and merging values - Latest survives.

 Block Size.

 1. SST Files - use block compression
 Larger block size - slower to read but better compression, smaller indexes. The default block size is too small 4kib, hence better to have between 16 - 256kib.
The biggest lever for read/write/space tuning.

2. SSTable Index 
Each SST has sparse in memory index pointing at each block - it allows for efficient seeking.

Bloom filters - Ribbon filters.

Embedded in SST files and not on index.
They are probabilistic data structures used to test if an element  definitely exists or likley exists. 
Enable with `setFilterPolicy(BloomFilter(bloomBitsPerKey))`
bloomBitsPerKey = 10 // ~1% false positives
Used only for point lookups not for iterators

Write Path -
  WAL - Write ahead log - Used for recontructing after a crash.
  For emphimeral rocksdb instances - Disable ths feature.
  Every write flushed to disk before returning.
  writeOptions().setDiableWal(true)

  Memtable - WriteBufferSize - default 64MiB 
  Commonly set b/w 16 to 256 MiB. In-memory data structure that holds data before itâ€™s flushed into an SStable, the implementation may use a RB Tree, a skiplist, a HashLinkList. OpenMem table fills up it becomes immutable SStable	 and is replaced with new open Memtable.
  maxWriteBufferNumber Default 2 Commonly increased.
  Common implementation is SkipList(binary Search) by default
  Vector another alternative

  Level 0 - 

  Background threads flush filled up memtable to SST files. As the files are sorted and the String files are sorted the reads are also very fast. The write is very fast with sequential I/O
  Eash flushed memTable becomes new segement SST file in level 0
  But level 0 SST files can have overlapping key ranges and within level 0 the keys are not sorted.

  Level 1 - 

  Level0FileCompactionTrigger default = 4. Once 4 sst files are accumulated it trigger a compaction.
  If the writes are faster than you can slow down writes no. of files with below triggers 
  level0SlowdownWritesTrigger default 20 (stalling)
  level0StopWritesTrigger default 36 - Stopping writes

  Level 1..N
  Every level is 10 times larger than the level above it. We can control the overall size of files in each level below are are few settings
  1. maxBytesForLevelMultiplier default is 10
  maxBytesforLevelBase default 256 MiB - Commonly <= GiB
  L1 -> 256MiB, L2-> ~2.5 GiB, L3 -> ~25 GiB
  Good target is 20-40 files across all levels - These files use file handles and more the file handles the app starts to slow down. We dont want 1000 files.
  Compaction is handled by background threads


  Deletes creates tombstones

  Tombstones: the key associated with a special "tombstone" value  - Uses more space to add a marker value "tombstone" and those keys are deleted during compaction
  A DeleteRange operation can be more write/space efficient but it is less read efficient. 

 
Read Path - 1. looks at block cache, 2. Open memTable
 
 Block Cache - 
 It allocates block cache per column family - Inmemmory structure. It can be really large 
 setBlockCache(LRUCache(cacheSizeInBytes))  default 8MiB  is very tiny hence often set it at GiB's to get better performance.

 It holds uncompressed blocks from SST files. It can be a big performace boost to get hot keys out. Invalidated on the write of the key with the cache and the one used often is the LRU cache.
 setBlockCache(LRUCache(cacheSizeInBytes)) - 

 If the key is a miss within block cache it looks at the open memtable where we are writing. Next we look at the closed SST files. First all Level 0 files from newest to the oldest. Bloom filters speed this lookup. SST files in Level 0 have overlap keys

 Level 1-N

 Levels >0 do not have any overlap keys. Compaction or dedup is already complete. Determine the sst file that can have the key using the index within the bloom filter . If the key might be present in a sst file it decompresses the block and puts it into a block cache.


 SST file comression -

 Each sst file can have compression associated with it. Not worth comressing level 0, as it can cause write stalls as we want the level 0 files to be very very fast.

 Recommendation is to use zstd for the bottom level (best compression, more cpu) and lz4 for everything else.

 Rocks DB Stats about the size of each level - 

 rocksDb.getProperty(rockDb.getDefaultColumnFamily(), "rocksDb.stats")

 L0, L1, L2, Sum (How many files, what is the tital size written) values accross all levels.

 Compaction can be explicity requested at each level and it is run as a background process. 

 rocksDb.compactRange(rockDb.getDefaultColumnFamily())
 rocksDb.getProperty(rockDb.getDefaultColumnFamily(), "rocksDb.stats")

 Flushing a skiplist MemTable will remove redundant keys. If same keys are added to a memTable multiple times during the process of compaction all the redunant keys are removed and only the recent value to flushed to a SST file.

 Read/Write and Space amplification RUM Conjecture - wHITE PAPER

  Trade off should be made between Read, Update and Memory(Space) overheads. 
  1. Read + Update - Trade space for cheap reads and writes
  2. Memory + Update - Trade slow reads for fast writes and low space
  3. Read + Memory - Trade slow writes for fast reads and low space

  Rocks DB uses memory is 3 ways. Unlike JVM where you can define the memory, it is difficult to decern where the levers are to make it space efficient. (to make it not use too much memory)

  1. Block cache - Uncompressed block cache for read operations 
  2. Memtables - writes buffers that each new value is written 
  3. SST files which contains the indexes and bloom/ribbon filters to find keys within each file.

  All the above are inmemory and they are independent & per column family. But they can be consolidated and the indexes and filters into block cache and pin them so that they're high priority and they can be used to explicity allocate memory in GiB's for for my block cache and to make sure that  indexes and filters actually in fit within that one gig of byte of memory and don't leak and don't run the instance out of memory. This is a powerful technique.

  setBlockCache(LRUCache(1024 * 1024 * 1024))
  cacheIndexAndFilterBlocks = true
  cacheIndexAndFilterBlocksWithHighPriority = true
  pinL0FilterAndIndexBlocksInCache = true


  Memory Allocator - Default glibc memory allocator - It is terrible. Replace it with jemalloc, allocator gives me better memory usage and it is not subject to memory leaks. 
  # libjemalloc-dev is an alternative memory allocator that can be used by rocksdb applications
  # to enable, add this to a child Docker container that imports this docker container in a FROM, or export via environment.properties:
  # ENV LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so
  RUN apt-get update && apt-get install -y libjemalloc-dev rename wget

  https://blog.cloudflare.com/the-effect-of-switching-to-tcmalloc-on-rocksdb-memory-use/

  General recommendations -

  Use small keys as significant processing time is spent merging and sorting records by key
  Disable WAL - IF NOT REQUIRED
  Don't compress levle 0 but use zstd compression at bottom level
  eliminate serialization and deserialization wherever possible
  Watch stats for write stalls

  Batch writes are faster 
  Why enable WAL - Persistant disk - If app needs the data from the disk (Not required generally as rocksdb should not be used as source of truth)
  Why not hollow - If the data set size is lot less memory and your data needs a lot of transformation and run business logic without just a get. 
  Isolation no sharing. Split and divide work. Listen to data by partition and make sure that business logic works along those lines.
  
  


























